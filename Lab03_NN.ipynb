{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b703febb",
   "metadata": {
    "id": "b703febb"
   },
   "source": [
    "# Lab-03 Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d37c9ad",
   "metadata": {
    "id": "4d37c9ad"
   },
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd247956",
   "metadata": {
    "id": "fd247956",
    "outputId": "f7d0a7b0-92b6-42b7-8ffc-b6712e16c722"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images:  2177\n",
      "Total images path:  2177\n",
      "Total postures:  9\n",
      "Total images_pixels:  2177\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import cv2\n",
    "\n",
    "images = []\n",
    "pose_name = []\n",
    "images_path = []\n",
    "images_pixels = []\n",
    "labels = []\n",
    "\n",
    "dict = {}\n",
    "\n",
    "i=0\n",
    "dataPath = './yogaData'\n",
    "\n",
    "for directory in os.listdir(dataPath):\n",
    "    dirPath = os.path.join(dataPath, directory)\n",
    "    if os.path.isdir(dirPath):\n",
    "        pose_name.append(directory)\n",
    "        for img in os.listdir(dirPath):\n",
    "            if len(re.findall('.png',img.lower())) != 0 or len(re.findall('.jpg',img.lower())) != 0 or len(re.findall('.jpeg',img.lower())) != 0:\n",
    "                img_path = os.path.join(dirPath,img)\n",
    "                images.append(img)\n",
    "                images_path.append(img_path)\n",
    "                img_pix = cv2.imread(img_path,1)\n",
    "                images_pixels.append(cv2.resize(img_pix, (100,100)))\n",
    "                labels.append(i)\n",
    "\n",
    "    i = i+1\n",
    "\n",
    "print(\"Total images: \", len(images))\n",
    "print(\"Total images path: \", len(images_path))\n",
    "print(\"Total postures: \", len(pose_name))\n",
    "print(\"Total images_pixels: \", len(images_pixels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0dc133df",
   "metadata": {
    "id": "0dc133df"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "shuffled = list(zip(images_pixels,labels))\n",
    "random.shuffle(shuffled)\n",
    "\n",
    "train_data, labels_data = zip(*shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a33ce89c",
   "metadata": {
    "id": "a33ce89c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([210., 268.,   0., 197., 209., 274., 198., 226., 334., 261.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "X_data = np.array(train_data)/255\n",
    "Y_data =  to_categorical(labels_data, num_classes = 10) # Should be 9 classes\n",
    "sum(Y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "07883caa-08cd-46f8-9921-ccf83eef6a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(Y_data)\n",
    "Y_data = np.delete(Y_data, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "07d8ab0e-112a-444c-b8f2-3eed9e35659d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2177.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(sum(Y_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fac3aa7",
   "metadata": {
    "id": "6fac3aa7"
   },
   "source": [
    "## Split the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "af66b307",
   "metadata": {
    "id": "af66b307"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, Y_data, test_size=0.2, random_state = 123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "67bcf119-4c62-44ad-8d63-65e9d435ee46",
   "metadata": {
    "id": "67bcf119-4c62-44ad-8d63-65e9d435ee46",
    "outputId": "a6558029-5223-4d2c-c2e0-39f303f94526"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1741, 100, 100, 3)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4380e53f",
   "metadata": {
    "id": "4380e53f"
   },
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7da20ba",
   "metadata": {
    "id": "f7da20ba",
    "outputId": "765a0fd3-57f1-4888-c994-4b5c40a5f79d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(max_features=None, n_jobs=-1, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_features=None, n_jobs=-1, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(max_features=None, n_jobs=-1, random_state=42)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Instantiate the RandomForestClassifier\n",
    "rf_model = RandomForestClassifier(\n",
    "    criterion='gini',          # i. criterion: The function to measure the quality of a split. 'gini' for Gini impurity or 'entropy' for information gain.\n",
    "    min_samples_split=2,        # ii. min_samples_split: The minimum number of samples required to split an internal node.\n",
    "    min_samples_leaf=1,         # iii. min_samples_leaf: The minimum number of samples required to be at a leaf node.\n",
    "    max_features=None,        # iv. max_features: The number of features to consider when looking for the best split. None means all features are considered.\n",
    "    bootstrap=True,             # v. bootstrap: Whether to use bootstrapping (sampling with replacement) when building trees.\n",
    "    random_state=42,             # vi. random_state: Seed used by the random number generator for reproducibility.\n",
    "    n_jobs = -1\n",
    ")\n",
    "\n",
    "# Fit the model to the training data\n",
    "nsamples, nx, ny, nrgb = X_train.shape\n",
    "X_train2 = X_train.reshape((nsamples, nx * ny * nrgb))\n",
    "rf_model.fit(X_train2, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cda0943-17cf-4a1a-9ec4-a2a35e3c4ad6",
   "metadata": {
    "id": "5cda0943-17cf-4a1a-9ec4-a2a35e3c4ad6"
   },
   "outputs": [],
   "source": [
    "nsamples, nx, ny, nrgb = X_test.shape\n",
    "x_test = X_test.reshape((nsamples, nx * ny * nrgb))\n",
    "\n",
    "predictions = rf_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d5f884-4c47-418f-a231-b4e97392b274",
   "metadata": {
    "id": "f1d5f884-4c47-418f-a231-b4e97392b274",
    "outputId": "ff9ee5ec-90bd-4b63-e3a5-09495bf6cfe5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44954128440366975\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.32      0.48        57\n",
      "           1       1.00      0.35      0.52        62\n",
      "           2       0.91      0.34      0.50        29\n",
      "           3       1.00      0.45      0.62        44\n",
      "           4       1.00      0.50      0.67        28\n",
      "           5       1.00      0.35      0.52        49\n",
      "           6       0.85      0.44      0.58        63\n",
      "           7       1.00      0.42      0.59        64\n",
      "\n",
      "   micro avg       0.96      0.39      0.56       396\n",
      "   macro avg       0.97      0.40      0.56       396\n",
      "weighted avg       0.97      0.39      0.56       396\n",
      " samples avg       0.36      0.36      0.36       396\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Icego\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Icego\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(accuracy)\n",
    "\n",
    "report = classification_report(y_test, predictions)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2e13c5-8882-4991-ab6b-6fe3798217be",
   "metadata": {
    "id": "3d2e13c5-8882-4991-ab6b-6fe3798217be",
    "outputId": "6bee5147-5648-466a-91b4-ddf75fcdbe0b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "234.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It looks to me like the model is just a little timid, it's got very high precision, so when it's guesssing it is pretty accurate\n",
    "# unfortunately, it didn't guess at all on 234 of the ~400 test cases\n",
    "sum(sum(y_test)) - sum(sum(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f128483b",
   "metadata": {
    "id": "f128483b"
   },
   "source": [
    "## Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ba27f0",
   "metadata": {
    "id": "61ba27f0"
   },
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a14e5ba-0329-47ba-990f-8777078315f4",
   "metadata": {
    "id": "7a14e5ba-0329-47ba-990f-8777078315f4",
    "outputId": "013bc5a4-181a-4465-957e-4fca627761bd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5a8d4937",
   "metadata": {
    "id": "5a8d4937",
    "outputId": "6fa494f5-f2d0-459d-9596-2ae8f5438a41"
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, Y, transform=None):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_sample = self.X[idx]\n",
    "        y_sample = self.Y[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            x_sample = self.transform(x_sample)\n",
    "\n",
    "        return x_sample, y_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "696b8df5",
   "metadata": {
    "id": "696b8df5"
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Converts the image to a PyTorch tensor\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c2148485",
   "metadata": {
    "id": "c2148485",
    "outputId": "c48931d4-100b-4b7d-9e6d-65c7d6a91a85"
   },
   "outputs": [],
   "source": [
    "train_ds = CustomDataset(X_train, y_train, img_transform)\n",
    "val_ds = CustomDataset(X_test, y_test, img_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "af4e3d07-850a-476a-940f-2af60ee5e1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = DataLoader(train_ds, batch_size = 16)\n",
    "val = DataLoader(val_ds, batch_size = 16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "00bfcea0",
   "metadata": {
    "id": "00bfcea0"
   },
   "outputs": [],
   "source": [
    "# We have to transform images beccause they're are an extremely high dimensional space,\n",
    "# if we transform it to a tensor it's just a 1 dim array which the computer likes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2959fb",
   "metadata": {
    "id": "6a2959fb"
   },
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "cffee568-7146-4dc8-9eec-29f058845d0a",
   "metadata": {
    "id": "cffee568-7146-4dc8-9eec-29f058845d0a"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.cnn_layers = nn.Sequential(\n",
    "            # in channels: # colours in input image (colour = 3)\n",
    "            # out channels: # filters\n",
    "            # kernel size: how big an area should we consider?\n",
    "            # stride: # of pixels to move the kernel over by\n",
    "            # padding: # of pixels to add at each edge of image (3x3 kernel : padding = 1 || 5x5 kernel: padding: 2)\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.linear_layers = nn.Sequential(nn.Linear(10000,9))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        output = self.cnn_layers(X)\n",
    "        output = output.view(output.size(0), -1)\n",
    "        output = self.linear_layers(output)\n",
    "        return output\n",
    "# Source: The Honorable Dr. Professor Mr. David Guy Brizan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "7735ad9d-f123-4d4b-ac76-f23cb81e09b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 1: 3 input, 16 output channels (also known as filters), a kernel size of 3x3, a stride of 1 \n",
    "#        (how much the filter moves each time), and padding of 1 (zero-padding to the input).\n",
    "# Layer 2: This layer exists to Normalize the inputs to the next Conv layer, \n",
    "#           BatchNorm is meant to accelerate training and make results more consistent\n",
    "# Layer 3: This activation layer breaks up the linear structure of outputs, and will squeeze outputs\n",
    "#          All negative inputs will become 0\n",
    "# Layer 4: MaxPool takes a 2x2 filter and returns only the max value for each square, this simplifies \n",
    "#          what we're working with, \"Down-sampling\"\n",
    "# Layer 5: Goes from high dimensional inputs/ what we're working with in the network and squeezes it \n",
    "#          into the output array, length 9 with corresponding probabilites for each pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "5b094240-8e30-4832-bb53-5b447647b584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why is the number of channels 3 for the first Convolutional layer? \n",
    "#    We have RGB images, so each channel will be responsible for one color\n",
    "# Why is the first parameter 10,000 for the Linear layer?\n",
    "#    Our images are 100x100, so the linear layer is responsible for taking that \n",
    "#    and squeezing it down to our output's size, a array of len 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ef75e302-33c3-41d4-8fd8-91cf20f159ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers exist to nudge the model in the right direction for the most accurate predictions\n",
    "# I like adamax, so I'll use that one \n",
    "# lr gives us a scalar multiple to reduce adjustments our optimizer gives so that we move slowly\n",
    "#    and carefully towards the best possible model and avoid bouncing around never finding the best one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "950da72b-27fa-4b62-9188-5a35e0898aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (cnn_layers): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (linear_layers): Sequential(\n",
      "    (0): Linear(in_features=10000, out_features=9, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = CNN().to(dtype=torch.float32)\n",
    "\n",
    "optimiser = optim.Adamax(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss() #we're using CrossEntropyLoss as our optimizer\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "fc9aa3b3-c769-4dae-9a0a-fa8cc287778f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 0.05568740761693601\n",
      "Epoch 1 loss: 0.06047326893921634\n",
      "Epoch 2 loss: 0.05417832665982733\n",
      "Epoch 3 loss: 0.057739354934919315\n",
      "Epoch 4 loss: 0.052631065275075786\n",
      "Epoch 5 loss: 0.05485115478716688\n",
      "Epoch 6 loss: 0.04982763051627836\n",
      "Epoch 7 loss: 0.051324222004041076\n",
      "Epoch 8 loss: 0.047231105551414125\n",
      "Epoch 9 loss: 0.048775000500902874\n",
      "Epoch 10 loss: 0.044283013365769744\n",
      "Epoch 11 loss: 0.049671571185906344\n",
      "Epoch 12 loss: 0.04299164980803744\n",
      "Epoch 13 loss: 0.04556016820811863\n",
      "Epoch 14 loss: 0.04113416861606027\n",
      "Epoch 15 loss: 0.04435534023577262\n",
      "Epoch 16 loss: 0.03997006377957146\n",
      "Epoch 17 loss: 0.042549457865630476\n",
      "Epoch 18 loss: 0.039060305868096036\n",
      "Epoch 19 loss: 0.04024390020907575\n",
      "Epoch 20 loss: 0.03650630478119187\n",
      "Epoch 21 loss: 0.037974757093769594\n",
      "Epoch 22 loss: 0.034186421010275844\n",
      "Epoch 23 loss: 0.036214417847377514\n"
     ]
    }
   ],
   "source": [
    "epochs = 24\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.\n",
    "    for images, labels in train:\n",
    "        optimiser.zero_grad()\n",
    "        output = model(images.to(dtype=torch.float32))\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        total_loss += loss.item()\n",
    "    print('Epoch', epoch, 'loss:', (total_loss / len(train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "6ab29779-7254-4755-b024-de87c6db7293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "315/436, accuracy: 0.7224770642201835\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in val:\n",
    "    optimiser.zero_grad()\n",
    "    outputs = model(images.to(dtype=torch.float32))\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    for i in range(len(labels)):\n",
    "        pred = predicted[i]\n",
    "        if labels[i][pred] == 1:\n",
    "            correct += 1\n",
    "            total +=1\n",
    "        else:\n",
    "            total+=1\n",
    "        \n",
    "print(f'{correct}/{total}, accuracy: {correct/total}')\n",
    "# CNN accuracy is significantly better than the RandomForest, also runs way faster\n",
    "# RFC accuracy was 0.44954128440366975"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
